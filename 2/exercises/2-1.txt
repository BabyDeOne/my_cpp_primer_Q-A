What are the difference between int, long, long long, and short?
Between an unsigned and a signed type?
Between a float and a double?

Answer:
    int: integer, 16 bits
    long: long integer, 32 bits
    long long: long integer too, 64 bits
    A signed type represents nagetive or positive numbers(includeing zero); An unsigned type represents only velues greater than or equal to zero.
    float: single-precision floating-point, 6 significant digits
    double: double precision floating-point, 10 significant digits